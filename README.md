# codex-cli-wrapper
wrapper script/application to codex-cli for enabling usage locally with for example Ollama.

## Requirements
Codex-cli application: https://github.com/openai/codex

## Main purpose
The purpose of this script/application is to simplify the usage of codex-cli, enable advance usage, and ensure that it also always executes the command responses generated from the used model, not just proposing them. 

## Default setup
The default setup is to use a local instance of ollama and it's models instead of default OpenAI, while still enabling the cli functionality and respons handling. 

